\chapter{Introduction}


\section{Motivation}
Distributed control of multiple unmanned autonomous robots (UARs) has been the subject of intensive research in recent years due to the potential applications in search and rescue missions, surveillance and reconnaissance, environmental sensing and monitoring, intelligent transportation and threat isolation/evasion, and obstacle evasion [1]. There are many practical commercial, civil and military applications that may benefit from advancements in UAR control strategies and algorithms from this area of research. 

\section{Related work}
Methods for successfully coordinating multiple robots to autonomously encircle a target point have been put forth by several research papers.  These distributed controls assume communication the robots to communicate their kinematic state variables between one another, and consequently researchers have designed their algorithms in ways that minimize the number of communication channels required between robots.  However, there has not been nearly as much research done that attempt to implement coordinated encirclement that rely entirely on each individual robot’s vision sensors.

A review of prior work at Bradley and research literature was conducted by our team during the conception phase of the project.  This included a review of e prior Class of 2016 Senior Project, Cooperative Control of Heterogeneous Mobile Robots Network [cite ref 1?]. The 2016 research and capstone project experience using the QBot2 robots was leveraged in our project. The 2016 project tested cooperative control using the QBot2 platform including the Qbot2 Camera and the Kinect RGBD camera for localization.  Wiﬁ communication was used to exchange position information among robots. Consensus and formation stabilization problems were studied. 

Our project also researched and references another work, highly relevant to our 2017 project objectives. Decentralized multi-robot encirclement of a 3D target with guaranteed collision avoidance for object tracking and encirclement by Franchi, Stegagno, and Oriolo, [cite ref 2]. Their work on 3-D (and 2-D) and distributed collaborative control is extensive and leverages research going back to 2001.   Their findings delivers a control framework for achieving encirclement of a target moving in a 3D space using multiple robots, image-only sensing, and requiring local communication between exactly two other members of the encircling group of robots. 

An additional consideration, for a vision-only sensing strategy was image processing techniques and algorithms.  We relied on Peter Corke’s book “Robotics, Vision and Control” as well as his Matlab toolbox for much of our image processing.  The academic tutorials packaged with the Qbot2 were also valuable resources.  It is worth mentioning that systems in the literature utilizing visual-only sensing generally assumed 360-degree robot sensing (e.g. fisheye camera). Ref 1 Research (extracts below) utilized a 240deg field of view sensor and a laser range finder for identification and locating targets to support inputs to their control strategies.  Because the Qbot2 cameras have less than a 90-degree field-of-view, our design would have to incorporate additional logic to locate targets outside the robot’s visual range.
